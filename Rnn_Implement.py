# -*- coding: utf-8 -*-
"""rnn_july_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yrXYFSOZDsDtaPmVxcLKwSgwEeGZ6e4d
"""

import pandas as pd
from pandas import to_numeric
import numpy as np
from keras import models
from keras import layers
from keras.layers import  LSTM, Dense, Dropout, SimpleRNN
from keras.models import Sequential
from keras.utils import to_categorical
from keras.layers import Dropout
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from matplotlib import pyplot as plt
import keras.utils as ku 
from keras.preprocessing.sequence import pad_sequences
from keras.callbacks import CSVLogger


csv_logger = CSVLogger('training_8.log', separator=',', append=False)

# load time_step_data
time_step_data = np.load('time_step_data_all.npy')


# load labels_data
labels_data = np.load('labels_data_all.npy')
#print(time_step_data.shape)

time_step_data.reshape(time_step_data.shape[0], time_step_data.shape[1], time_step_data.shape[2])

"""** Label Data **"""

#print(labels_data.shape)

labels_data_categorical = ku.to_categorical(labels_data, num_classes = 2)
labels_data_categorical.astype(np.int)

# Split the data into training and testing sets
train_X, test_X, train_labels, test_labels = train_test_split(time_step_data, 
                                                              labels_data_categorical, 
                                                              test_size = 0.2,
                                                              random_state = 0)

print('Training Features Shape:', train_X.shape)
print('Training Labels Shape:', train_labels.shape)
print()
print('Testing Features Shape:', test_X.shape)
print('Testing Labels Shape:', test_labels.shape)


def dataset_preparation_rnn_lstm(__input_sequences__, __labels__):
  num_of_classes = 2
  input_sequences = __input_sequences__
  input_sequences = input_sequences.reshape(input_sequences.shape[0], input_sequences.shape[1], input_sequences.shape[2])
  labels = __labels__
  total_time_steps = __input_sequences__.shape[1]
  #labels = ku.to_categorical(labels, num_classes = num_of_classes)
  return input_sequences, labels, total_time_steps, num_of_classes


input_sequences, input_labels, total_time_steps, num_of_classes = dataset_preparation_rnn_lstm(train_X, train_labels)

print(input_sequences.shape)
print(input_labels.shape)
print(total_time_steps)
print(num_of_classes)

def create_model_rnn(input_sequences, label, total_time_steps, num_of_classes):
    model = Sequential()
    model.add(SimpleRNN(units=254, input_shape=(input_sequences.shape[1], input_sequences.shape[2]),
                                              return_sequences=False))
    model.add(Dense(num_of_classes, kernel_initializer='random_uniform', activation='sigmoid'))
    model.compile(loss='categorical_crossentropy', optimizer='Adadelta', metrics=['accuracy'])
    history = model.fit(input_sequences, label, callbacks=[csv_logger], epochs=20, verbose=1,
                        validation_split = 0.2)
    model.save("Simple_RNN_1000.h5")
    print(model.summary())
    return model, history

model, history = create_model_rnn(input_sequences, input_labels, total_time_steps, num_of_classes)
#plot accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model acc')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

#plot loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""**Basic LSTM**"""

def create_model_lstm(input_sequences, label, total_time_steps, num_of_classes):
    print(input_sequences.shape)
    print(label.shape)
    model = Sequential()
    model.add(LSTM(256, input_shape=(input_sequences.shape[1], input_sequences.shape[2]), return_sequences = False))
    #model.add(Dropout(0.2))
    #model.add(LSTM(128))
    model.add(Dense(num_of_classes, kernel_initializer='random_uniform', activation='sigmoid'))
    model.compile(loss='categorical_crossentropy', optimizer='Adadelta', metrics=['accuracy'])
    history = model.fit(input_sequences, label, callbacks=[csv_logger], epochs=5, verbose=1,
                        validation_split = 0.2)
    # serialize weights to HDF5
    model.save("Simple_RNN_Port_Occupancy.h5")
    print(model.summary())
    return model, history

model, history = create_model_lstm(input_sequences, input_labels, total_time_steps, num_of_classes)

#plot accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model acc')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

#plot loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()